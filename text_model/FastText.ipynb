{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIM3AQZGeNQ_",
    "outputId": "9cd681d2-efdc-4f72-cd39-8411d6c831d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keybert\n",
      "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting sentence-transformers>=0.3.8 (from keybert)\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.22.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.3.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.14.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.15.2+cu118)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.8.1)\n",
      "Collecting sentencepiece (from sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.12.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2023.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (23.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (16.0.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.10.31)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (8.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (2.1.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Building wheels for collected packages: keybert, sentence-transformers\n",
      "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23777 sha256=ce25b4397af80c147f165563631ec6bc2f5f3fcebc2813f2db40f1b5d002a09c\n",
      "  Stored in directory: /root/.cache/pip/wheels/66/8d/e6/b0e2f8d883b0fd51819226f67ad9843e04913ce4a97241ff4b\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=40cd748f45876ee6a609db4153465e715a73df70acf63e46a5168bc2dc04af83\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
      "Successfully built keybert sentence-transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers, keybert\n",
      "Successfully installed huggingface-hub-0.14.1 keybert-0.7.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "LkFR29mJe1Jl",
    "outputId": "4ef16216-a973-48a6-fdb4-c81f6f5c7794",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting kiwipiepy\n",
      "  Downloading kiwipiepy-0.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dataclasses (from kiwipiepy)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting kiwipiepy-model~=0.15 (from kiwipiepy)\n",
      "  Downloading kiwipiepy_model-0.15.0.tar.gz (30.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from kiwipiepy) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kiwipiepy) (4.65.0)\n",
      "Building wheels for collected packages: kiwipiepy-model\n",
      "  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.15.0-py3-none-any.whl size=30602629 sha256=ae0ca16f661717ea9922feb6a98090db73d1b3718687b9b2be6928d2336ae5da\n",
      "  Stored in directory: /root/.cache/pip/wheels/f3/55/41/ca474338ece1bc4314b01445f64ff002d71e19df45575a16e2\n",
      "Successfully built kiwipiepy-model\n",
      "Installing collected packages: kiwipiepy-model, dataclasses, kiwipiepy\n",
      "Successfully installed dataclasses-0.6 kiwipiepy-0.15.1 kiwipiepy-model-0.15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "dataclasses"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kiwipiepy\n",
      "  Downloading kiwipiepy-0.15.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/ines/anaconda3/lib/python3.9/site-packages (from kiwipiepy) (4.64.0)\n",
      "Collecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting kiwipiepy-model~=0.15\n",
      "  Downloading kiwipiepy_model-0.15.0.tar.gz (30.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.5 MB 12.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ines/anaconda3/lib/python3.9/site-packages (from kiwipiepy) (1.21.5)\n",
      "Building wheels for collected packages: kiwipiepy-model\n",
      "  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.15.0-py3-none-any.whl size=30602642 sha256=3cc3acb8b36cb0147d6af860ba2dab10a7b36001fe90933f2cac8f601d688fb9\n",
      "  Stored in directory: /home/ines/.cache/pip/wheels/9e/34/31/a1682c8249bf2ba89a29498d62c35c08ceb116537b4530d93e\n",
      "Successfully built kiwipiepy-model\n",
      "Installing collected packages: kiwipiepy-model, dataclasses, kiwipiepy\n",
      "Successfully installed dataclasses-0.6 kiwipiepy-0.15.1 kiwipiepy-model-0.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "riqg4pRDfCpw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from kiwipiepy import Kiwi\n",
    "from transformers import BertModel\n",
    "\n",
    "text = 'padding 어떨때 하는지 얼만큼 하는지 꼭 물어보기'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1X19YnIgdMd"
   },
   "source": [
    "- keyphrase_ngram_range:몇 개의 ngram으로 사용할 것인가\n",
    "- top_n: 몇 개의 키워드를 뽑을 것인가\n",
    "- stop_words: 어디서 멈출 것인가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxNtz8LzfkmN",
    "outputId": "d994622a-02eb-46ea-c90e-e6c7e17d024a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('하는지', 0.8421),\n",
       " ('어떨때', 0.8408),\n",
       " ('물어보기', 0.6987),\n",
       " ('얼만큼', 0.6909),\n",
       " ('padding', 0.5416)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "kw_model =KeyBERT(model)\n",
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,1), stop_words=None, top_n=10)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jg-xUN4gyzO"
   },
   "source": [
    "## Text 카테고리화 part <<여기부터\n",
    "- 앞으로 할 일: 데이터 수집\n",
    "- src: https://raspberrypi98.tistory.com/127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YM6hPbL8pNYQ",
    "outputId": "82a9aca2-61b8-447d-aaa2-4784ddffb690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting PyKomoran\n",
      "  Downloading PyKomoran-0.1.6.post1-py3-none-any.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: py4j, PyKomoran\n",
      "Successfully installed PyKomoran-0.1.6.post1 py4j-0.10.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyKomoran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yBfQGPEpqwfc",
    "outputId": "33b7874d-941d-471d-b7dd-3c5f03ab13dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "슈크림/NNP 빵/NNG 구매/NNG 하/XSV 지/EC\n",
      "['슈크림', '빵', '구매']\n",
      "['슈크림', '빵', '구매']\n"
     ]
    }
   ],
   "source": [
    "from PyKomoran import *\n",
    "text = '슈크림빵 구매하기'\n",
    "komoran = Komoran('EXP')\n",
    "# 입력 문장을 형태소 별로 나누어 {형태소/품사} 형태로 태깅된 결과\n",
    "print(komoran.get_plain_text(text))\n",
    "# get_nouns(): 입력 문장에서 명사만 추출\n",
    "print(komoran.get_nouns(text))\n",
    "# get_morphes_by_tags(): 입력된 문장에서 주어진 품사들만 추출\n",
    "print(komoran.get_morphes_by_tags(text, tag_list=['NNP','NNG','SL','VV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Q2zikxbDcoR",
    "outputId": "45bcb457-3313-4471-8d07-7f10f04de5d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 4.6 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pybind11>=2.2\n",
      "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/ines/anaconda3/lib/python3.9/site-packages (from fasttext) (61.2.0)\n",
      "Requirement already satisfied: numpy in /home/ines/anaconda3/lib/python3.9/site-packages (from fasttext) (1.21.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.2-cp39-cp39-linux_x86_64.whl size=310872 sha256=ac08a8d865604b0e6a4ef32db380f560cad8eb5be2879fb14d711deee5c094bb\n",
      "  Stored in directory: /home/ines/.cache/pip/wheels/64/57/bc/1741406019061d5664914b070bd3e71f6244648732bc96109e\n",
      "Successfully built fasttext\n",
      "Installing collected packages: pybind11, fasttext\n",
      "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
     ]
    }
   ],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def train_test_split_file(input_path: str,\n",
    "                          output_path_train: str,\n",
    "                          output_path_test: str,\n",
    "                          test_size: float,\n",
    "                          random_state: int=1234,\n",
    "                          encoding: str='utf-8',\n",
    "                          verbose: bool=True):\n",
    "    random.seed(random_state)\n",
    "\n",
    "    # we record the number of data in the training and test\n",
    "    count_train = 0\n",
    "    count_test = 0\n",
    "    train_range = 1 - test_size\n",
    "\n",
    "    with open(input_path, encoding=encoding) as f_in, \\\n",
    "         open(output_path_train, 'w', encoding=encoding) as f_train, \\\n",
    "         open(output_path_test, 'w', encoding=encoding) as f_test:\n",
    "\n",
    "        for line in f_in:\n",
    "            random_num = random.random()\n",
    "            if random_num < train_range:\n",
    "                f_train.write(line)\n",
    "                count_train += 1\n",
    "            else:\n",
    "                f_test.write(line)\n",
    "                count_test += 1\n",
    "\n",
    "    if verbose:\n",
    "        print('train size: ', count_train)\n",
    "        print('test size: ', count_test)\n",
    "\n",
    "\n",
    "def prepend_file_name(path: str, name: str) -> str:\n",
    "    \"\"\"\n",
    "    e.g. data/cooking.stackexchange.txt\n",
    "    prepend 'train' to the base file name\n",
    "    data/train_cooking.stackexchange.txt\n",
    "    \"\"\"\n",
    "    directory = os.path.dirname(path)\n",
    "    file_name = os.path.basename(path)\n",
    "    return os.path.join(directory, name + '_' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "test_size = 0.2\n",
    "input_path = os.path.join(data_dir, 'cooking.stackexchange.txt')\n",
    "input_path_train = prepend_file_name(input_path, 'train')\n",
    "input_path_test = prepend_file_name(input_path, 'test')\n",
    "random_state = 1234\n",
    "encoding = 'utf-8'\n",
    "\n",
    "train_test_split_file(input_path, input_path_train, input_path_test,\n",
    "                      test_size, random_state, encoding)\n",
    "print('train path: ', input_path_train)\n",
    "print('test path: ', input_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "DPWgK9Ih2tGb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Read 0M words\n",
      "Number of words:  495\n",
      "Number of labels: 36\n",
      "\r",
      "Progress: 100.4% words/sec/thread:  152068 lr: -0.003947 avg.loss:  1.564982 ETA:   0h 0m 0s\r",
      "Progress: 100.0% words/sec/thread:  151849 lr:  0.000000 avg.loss:  1.564982 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised(input='/home/ines/Desktop/memo_data',\n",
    "                                  epoch=100,\n",
    "                                  bucket=20000,\n",
    "                                  lr=1,\n",
    "                                  wordNgrams=2,\n",
    "                                  dim=80,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "G9X7VJ58FeC9",
    "outputId": "5a889c8c-abf5-4a62-95b7-e28eb2124d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "왕거미/NNG 식당/NNG\n"
     ]
    }
   ],
   "source": [
    "from PyKomoran import *\n",
    "text = '왕거미식당'\n",
    "komoran = Komoran('EXP')\n",
    "# 입력 문장을 형태소 별로 나누어 {형태소/품사} 형태로 태깅된 결과\n",
    "print(komoran.get_plain_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IgQ7DcKpD5cO"
   },
   "outputs": [],
   "source": [
    "model.save_model('./v2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/ines/anaconda3/lib/python3.9/site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/ines/anaconda3/lib/python3.9/site-packages (from konlpy) (4.8.0)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Using cached JPype1-1.4.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
      "Requirement already satisfied: packaging in /home/ines/.local/lib/python3.9/site-packages (from JPype1>=0.7.0->konlpy) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ines/anaconda3/lib/python3.9/site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.4)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성 수 일미 락\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvAT_JRj9Qcc",
    "outputId": "986b48f2-3965-42d5-8e49-d56f8dd56942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__요리', '__label__1.할일', '__label__3.기억해야할것'),\n",
       " array([0.29509467, 0.22729391, 0.19884619]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "memo = okt.morphs('냉동 새우 해동')\n",
    "pix = \" \".join(memo[i] for i in range(len(memo)))\n",
    "\n",
    "\n",
    "model.predict(pix,k=3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
